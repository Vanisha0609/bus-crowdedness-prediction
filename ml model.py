# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nhuTrboL44uKkfQzCnmUPMA4UDWaOKSH
"""

import pandas as pd
import numpy as np

# Set a random seed for reproducibility
np.random.seed(42)

days = pd.date_range(start='2023-01-01', periods=365, freq='D')

#bus ridership
bus_ridership = np.random.randint(10000, 50000, size=len(days))  # Passengers per day
metro_ridership = np.random.randint(15000, 60000, size=len(days))

# Weather data
temperature = np.random.uniform(25, 40, size=len(days))  # Degrees Celsius
humidity = np.random.uniform(50, 90, size=len(days))      # Percentage
rainfall = np.random.uniform(0, 50, size=len(days))       # Millimeters of rain

# Traffic data
traffic_congestion = np.random.randint(1, 10, size=len(days))  # 1: Low, 10: High

# Route data:
bus_routes = np.random.randint(1, 50, size=len(days))  # Route number
avg_speed = np.random.uniform(20, 50, size=len(days))  # Average speed in km/h
stops = np.random.randint(5, 30, size=len(days))       # Number of stops

# Create the DataFrame
data = {
    'date': days,
    'bus_ridership': bus_ridership,
    'metro_ridership': metro_ridership,
    'temperature': temperature,
    'humidity': humidity,
    'rainfall': rainfall,
    'traffic_congestion': traffic_congestion,
    'bus_route': bus_routes,
    'avg_speed': avg_speed,
    'num_stops': stops
}

df = pd.DataFrame(data)

# Save the dataset to CSV
df.to_csv('chennai_public_transport_data.csv', index=False)

# Display the dataset (optional)
df.head()

import joblib

df = pd.read_csv('chennai_public_transport_data.csv')


print(df.isnull().sum())


df['date'] = pd.to_datetime(df['date'])

# Set a threshold for 'crowded' (you can adjust this based on domain knowledge)
threshold = 30000  # Example threshold for crowdedness

# Create a binary target column (1 = Crowded, 0 = Not Crowded)
df['crowded'] = df['bus_ridership'].apply(lambda x: 1 if x > threshold else 0)

# Feature Engineering: Add day of the week and weekend flag
df['day_of_week'] = df['date'].dt.dayofweek  # Monday=0, Sunday=6
df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)

# Normalize numerical features (optional)
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df[['temperature', 'humidity', 'rainfall', 'traffic_congestion', 'avg_speed', 'num_stops']] = scaler.fit_transform(
    df[['temperature', 'humidity', 'rainfall', 'traffic_congestion', 'avg_speed', 'num_stops']]
)

# Inspect the cleaned dataset
df.head()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

X = df[['temperature', 'humidity', 'rainfall', 'traffic_congestion', 'avg_speed', 'num_stops', 'is_weekend']]
y = df['crowded']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print('Confusion Matrix:\n', conf_matrix)
print('Classification Report:\n', class_report)
joblib.dump(clf, 'random_forest_model.pkl')
from google.colab import files
files.download('random_forest_model.pkl')

# Install streamlit in your environment: !pip install streamlit
!streamlit run your_script.py

!pip install streamlit